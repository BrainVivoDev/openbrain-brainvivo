<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Open Brain: Decoding Human Emotions Through Novel Brain Embedding">
  <meta name="keywords" content="OpenBrain, Brain Embedding, MRI, Emotion AI, Neuroscience, BrainVivo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open Brain: Decoding Human Emotions Through Novel Brain Embedding</title>

  <meta name="google-site-verification" content="6lbYN1vX7A4sD8SrVniq84UEKyEUSBgxeP7d3FjuuK0" />

  <!-- Open Graph (LinkedIn, Facebook) -->
  <meta property="og:title" content="Open Brain: Decoding Human Emotions Through Novel Brain Embedding" />
  <meta property="og:description" content="Explore how brain embeddings can decode emotional content in images using Open Brain Explorer." />
  <meta property="og:url" content="https://brainvivodev.github.io/openbrain-brainvivo/" />
  <meta property="og:image" content="https://brainvivodev.github.io/openbrain-brainvivo/assets/thumbnail.png" />
  <meta property="og:type" content="website" />

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Open Brain: Decoding Human Emotions Through Novel Brain Embedding" />
  <meta name="twitter:description" content="Explore how brain embeddings can decode emotional content in images using Open Brain Explorer." />
  <meta name="twitter:image" content="https://brainvivodev.github.io/openbrain-brainvivo/assets/thumbnail.png" />

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <!-- <link rel="icon" href="./static/images/icon.png"> -->
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="shortcut icon" href="path/to/favicon.ico" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</head>


<style>
  #main {
    position: relative;
    ;
    width: 1200px;
  }

  .box {
    float: left;
    padding: 15px 0 0 15px;
    /*        background-color: red;*/
  }

  .pic {
    width: 500px;
    padding: 10px;
    border: 1px solid #ccc;
    border-radius: 5px;
    background-color: #fff;
  }

  .pic img {
    width: 500px;
  }

  body {
    font-family: sans-serif;
    margin: 2rem;
  }

  .markdown {
    border: 1px solid #ccc;
    padding: 1rem;
    background: #f9f9f9;
  }

  .image-wrapper {
    text-align: center;
    margin-top: 20px;
  }

  .image-container {
    display: inline-flex;
    flex-direction: row;
    justify-content: center;
    align-items: flex-start;
    gap: 20px;
  }

  .image-container img {
    width: 300px;
    height: auto;
    display: block;
  }

  .caption {
    margin-top: 10px;
    font-size: 16px;
    color: #444;
  }
</style>



<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"></h1>
            <h2 class="title is-2 publication-title">Open Brain: Decoding Human Emotions Through Novel Brain Embedding</h2>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://brainvivo.com/" style="color:#030b09;font-weight:normal;">BrainVivo</a>
              </span>
            </div>

            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Open‑Brain is a novel brain-embedding library from BrainVivo that transforms visual features into a shared "Common‑Brain" cognitive space.</span>
              <span class="author-block">This novel cognitive space is generated from multiple recorded brain activations and visual data using BrainVivo's proprietary fMRI scans, yielding cognitive-aware vectors that mimic real-time emotional and perceptual responses.</span>
              <!--
           <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>Tencent AI Lab </span>
            -->
            </div>
            <div class="is-size-5 publication-authors">
              <!--
            <span class="author-block" style="font-size: 90%;"><sup>*</sup>Major contributors</span>
            <span class="author-block" style="font-size: 90%;"><sup>&#x2628;</sup>interns at Tencent AI Lab</span>
            -->
            </div>

            <br>


            <div class="column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="https://github.com/BrainVivoDev/OpenBrain" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://embedding-explorer.brainvivo.com" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-brain"></i>
                    </span>
                    <span>Brain Embedding Explorer (on-line)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://youtu.be/taKd48fv0cU?si=NI-tll0uwfkSErKs" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Brain Embedding Explorer (video)</span>



                    <span class="link-block">
                      <a href="https://demo.brainvivo.com/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-brain"></i>
                        </span>
                        <span>Perceptor (on-line)</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://youtu.be/YXTVy4QlaD4" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-youtube"></i>
                        </span>
                        <span>Perceptor (video)</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://discord.gg/QfX4PS79" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-discord"></i>
                        </span>
                        <span>Discord</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://brainvivo.com/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-globe"></i>
                        </span>
                        <span>BrainVivo Website</span>
                      </a>
                    </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <link rel="stylesheet" type="text/css" href="js/simple_style.css" />
  <script type="text/javascript" src="js/simple_swiper.js"></script>


  <section class="section">
    <div class="container is-max-desktop">

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Open-brain is trained on functional MRI data to predict the neural activation patterns that images elicit
              across 1,024 distinct brain regions. These predicted responses serve as brain embedding,
              encoding the emotional reactions each image evokes within a specific cohort.</b>
            </p>
            Features:
            <ul>
              <li>Emotion-Based Image Clustering: cluster images according to the emotional responses they elicit.</li>
              <li>Emotional Transformation: modify an image's emotional tone by adjusting activations in specific brain
                regions.</li>
              <li>Media Characterization: evaluate any visual content by analyzing the emotions it provokes.</li>
            </ul>
          </div>
        </div>
      </div>


      <br>
      <br>
      <br>

      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Brain embedding generation</h2>
          <div class="content has-text-justified">
            <p>
              Refer to Figure 1: visual stimuli (1) are first converted into image embeddings (2); these embeddings are paired with their measured brain responses using proprietary MRI scanning protocols (3) to train a supervised model that predicts the brain activation pattern (4) for any new image.
            </p>

            <img id="embedding_generation" width="1000" src="images/image_embedding_generation.svg">

            <div style="font-family: 'Times New Roman'; width: 100%; text-align: center;">

  <div style="max-width: 800px; margin: 0 auto;">
    <p style="font-weight: bold; font-size: 1.2em; margin-bottom: 0.3em;">
      Figure 1: Brain embedding creation process.
    </p>
  </div>

</div>

          </div>
        </div>
      </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">The Valence-Arousal plane and the emotional separation in brain-embedding space</h2>
          <div class="content has-text-justified">
            <p>
              The first shared human cognitive ability is emotion, characterized along two orthogonal dimensions:
 (1) Valence — how positive or negative the feeling is, and
 (2) Arousal — its intensity or activation level.
This framing traces back to Russell (1980), whose circumplex model of affect arranges emotion terms in a circular space defined by these axes. Building on that foundation, Bradley and Lang (1994) introduced the Self‑Assessment Manikin (SAM), a nonverbal pictorial scale in which simple cartoon figures convey gradations of pleasure (happy ↔ unhappy) and arousal (excited ↔ calm). Researchers have since extended the framework to language by compiling normative affective ratings for words. In a landmark effort, Warriner et al. (2013) collected valence and arousal judgments for nearly 14,000 English lemmas, producing a lexicon of average “pleasantness” and “intensity” scores. Figure 2 depicts the density of these words on the valence‑arousal plane, with select emotion‑related terms highlighted.

            </p>

          <div class="has-text-centered">
            <img id="embedding_generation" width="80%" src="images/valence_arousal_words.png">
          </div>

            <div style="font-family: 'Times New Roman'; width: 100%; text-align: center;">

  <div style="max-width: 800px; margin: 0 auto;">
    <p style="font-weight: bold; font-size: 1.2em; margin-bottom: 0.3em;">
      Figure 2: The density of words in Warriner et al. (2013) on the valence-arousal plane along with some highlighted words which are related to emotions.
    </p>
  </div>

</div>


            <p>In parallel, several standardized image databases have been developed to study emotional responses within
              the valence-arousal paradigm. The International Affective Picture System (IAPS; Lang et al., 1997) was
              among the first, offering over 1,100 photographs that span the spectrum from highly pleasant to intensely
              unpleasant, each accompanied by normative valence and arousal ratings. The Nencki Affective Picture System
              (NAPS; Marchewka et al., 2014) expanded this repertoire with 1,356 high-resolution images across
              categories such as people, animals, objects, and landscapes, all rated by hundreds of participants. More
              recently, the Open Affective Standardized Image Set (OASIS; Kurdi et al., 2017) introduced 900 color
              images covering a broad array of everyday themes, whose valence and arousal norms were collected via
              crowdsourcing.
            </p>

            <p>
              We employed the OASIS dataset to demonstrate open-brain's capabilities. For each image, we generated a
              predicted brain response vector—termed the “brain embedding”—by processing its ImageBind embedding through
              our model, yielding a 1,024-dimensional representation. Both the original ImageBind embeddings and the
              brain embeddings were then projected into two dimensions using tSNE. Each point in the resulting
              scatterplot was colored according to the image's mean valence and arousal scores from OASIS. Figure 3
              juxtaposes the tSNE visualization of brain embeddings with that of the raw image embeddings, revealing
              that the brain embeddings exhibit a more distinct separation of emotional categories.
            </p>

            <br>
            <br>

            <div class="columns is-centered has-text-centered">
              <img src="images/tsne_brain_embedding.png" alt="Image 1" width="500">
              <img src="images/tsne_imagebind_embedding.png" alt="Image 2" width="500">
            </div>

            <div class="columns is-centered has-text-centered">
              <img src="images/tsne_brain_embedding_perplex10.png" alt="Image 1" width="500">
              <img src="images/tsne_openclip_embedding_perplex10.png" alt="Image 2" width="500">
            </div>

            <!-- <img id="embedding_generation" width="80%" src="images/transformation_in_embedding_space.png"> -->

            
         <div style="font-family: 'Times New Roman'; width: 100%; text-align: center;">

  <div style="max-width: 800px; margin: 0 auto;">
    <p style="font-weight: bold; font-size: 1.2em; margin-bottom: 0.3em;">
      Figure 3: Two-dimensional tSNE visualizations for the OASIS image set.
    </p>
    <p style="font-size: 1em; line-height: 1.4; margin-top: 0;">
      Top Left: brain embeddings (perplexity=5). Top Right: ImageBind embeddings (perplexity=5). 
      Bottom Left: brain embeddings (perplexity=10). 
      Bottom Right: Open CLIP embeddings (perplexity=10). 
      Each point is colored according to its image's mean valence-arousal coordinates from the OASIS norms, 
      illustrating how emotional content clusters more distinctly in the brain-embedding space.
    </p>
  </div>

</div>

      <br>
      <br>

<div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Perceptor</h2>
          <div class="content has-text-justified">
            <p>
              Perceptor analyzes media (videos and images) content using a brain response prediction model (one which is
              adapted for continuous sttimuli). The tool helps content creators understand potential audience reactions
              through quantitative metrics.
            </p>

            Features:
            <ul>
              <li> Emotional Analysis: Temporal breakdown of emotional responses (sadness, happiness, etc.)</li>
              <li> Valence-Arousal Data: Plots showing emotional intensity and sentiment changes</li>
              <li> Perception Score: Numerical evaluation of content-persona alignment</li>
              <li> Brain Region Mapping: Visualization of activated brain regions</li>
              <li> LLM Integration: Text-based interface for content improvement questions</li>
              <li> Cognitive Dimension Analysis: Multi-factor evaluation of memory, attention, and decision-making</li>
              responses
            </ul>
          </div>
        </div>
      </div>
    </div>


    <br>
    <br>
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title has-text-centered">Perceptor (tutorial)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/YXTVy4QlaD4?si=1ZrvQts3MKdnfj_r" title="YouTube video player" frameborder="0" allow="accelerometer; 
              autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>


    <br>
    <br>

          </div>
        </div>
      </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Transformations in brain-embedding space</h2>
          <div class="content has-text-justified">
            <p>
              Image embedding spaces are learned by computer vision models to represent images in a compact, semantic
              form. In such spaces, each image is mapped to a point (vector) such that images with similar semantic
              content lie close together. Recent advances have shown that these embedding spaces not only capture
              high-level semantics but also support semantic transformations via simple vector operations. In other
              words, by moving in certain directions in the embedding space, one can meaningfully change attributes or
              concepts in the image representation. This phenomenon, often called embedding arithmetic, was first
              popularized in the word embedding domain (e.g., the famous example vector(“King”) – vector(“Man”) +
              vector(“Woman”) ≈ vector(“Queen”) ). Researchers have since explored analogous behavior in vision and
              vision-language models. Radford et al. (2015) observed that the latent space of a Generative Adversarial
              Network (GAN) learned on images supports arithmetic manipulations. In their DCGAN experiments, specific
              algebraic combinations of latent codes produced meaningful changes in the output image (e.g. arithmetic
              operations on face latent vectors could change attributes like adding glasses) . Subsequent research has
              formalized and expanded the discovery of semantic directions in image embeddings. For example,
              InterFaceGAN (Shen et al., 2020) showed that for pretrained GANs, one can find a linear boundary in latent
              space for attributes such as gender, age, or smile by training a simple classifier, and then use the
              normal to that boundary as a direction to manipulate the attribute in generated images . They found that
              latent spaces are surprisingly well-behaved: moving a latent code in the “gender” direction smoothly
              transitions a face from male to female, while keeping other aspects intact, up to a point . Similarly,
              Härkönen et al. (2020) introduced GANSpace, which uses Principal Component Analysis (PCA) on latent
              vectors to discover major axes of variation . These principal directions were found to correspond to
              semantic changes like lighting without any supervision.
            </p>

            <p> Our brain responds differently to each visual image. While all visual images increase neural activity in
              regions dedicated to visual processing, emotionally salient images also engage brain areas associated with
              emotion regulation and can shift attention toward the emotional content. Furthermore, two images of
              similar emotional intensity may elicit distinct patterns of neural activation depending on their valence
              (e.g., positive vs. negative). For example, an image that would cause a feeling of happiness in the
              viewer's brain, would elicit a different response in specific brain areas from an image that would cause a
              feeling of sadness.

              Building on these principles, the Brain Embedding Explorer allows users to upload an image, and transform
              its emotional profile - in a more positive or negative direction.
              This transformation is performed in the brain embedding space, by offsetting the brain embedding along a direction in a brain embedding space. The transformation vector itself is obtained by subtracting the average brain embeddings of low‑valence images from the average brain embeddings of high‑valence images. This operation eliminates cognitive activity associated with early visual processing, yielding a transformation vector that isolates perceptual emotional valence. Offsetting any brain embedding along this vector modulates perceived valence while leaving lower‑level visual representations largely unchanged. The transformation is depicted in Figure
              4.
            </p>

            <p>Unlike purely semantic embedding models such as CLIP or imagebind, which capture conceptual or
              content-based similarities between images, the transformation in brain embedding space directly alters how
              a human might perceive the emotional dimension of the image. Our method derives its transformation vector
              from actual neural responses (fMRI), thus reflecting not only the semantic aspects of the image but also
              the affective processes at play in the human brain. By harnessing this neural-level information, the Brain
              Embedding Explorer contributes something new: it provides a means to shift an image's perceived emotional
              quality, rather than merely rearranging its semantic attributes.
            </p>

            <img id="embedding_generation" width="1000" src="images/transformation_in_embedding_space.svg">

            <div style="font-family: 'Times New Roman'; width: 100%; text-align: center;">

  <div style="max-width: 800px; margin: 0 auto;">
    <p style="font-weight: bold; font-size: 1.2em; margin-bottom: 0.3em;">
      Figure 4: Transformation in brain-embedding space. (2) brain-embedding is calculated from image-embedding using Open Brain. (3a), (3c) the brain response is modified in certain areas using a pre-calculated transformation vector. (5a), (5b) query for similar images in brain-embedding space.
    </p>
  </div>

</div>

          </div>
        </div>
      </div>
    </div>

    <br>
    <br>

    <div class="container is-max-desktop">

     

    <br>
    <br>

    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Brain Embedding Explorer</h2>
          <div class="content has-text-justified">
            <p>
              The Brain Embedding Explorer app allows you to freely explore how images map onto brain embeddings. To get
              started, choose a LanceDB table (the default uses the OASIS dataset, as shown in the first screenshot).
              Next, upload an image and transform the emotion portrayed in the image by using the emotions slider, then
              click the “Modify Emotion” button. This transforms the brain embedding accordingly and displays five
              images
              from the OASIS dataset that share the most similar brain embedding (see second screenshot). Finally, delve
              deeper into the brain response by viewing which regions are most responsive and learning about their
              cognitive roles in perception (see third screenshot). These results can also be downloaded as a CSV file
              for
              further analysis.
            </p>

          </div>
        </div>
      </div>
    </div>


    <br>
    <br>
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title has-text-centered">Brain Embedding Explorer (tutorial)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/taKd48fv0cU?si=wvp78DqohMofXoMG" title="YouTube video player" frameborder="0" allow="accelerometer; 
              autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>


  </section>



  <script src="js/Underscore-min.js"></script>
  <script src="js/index.js"></script>


  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">References</h2>
          <div class="content has-text-justified">
            <ul>
              <li>
                Russell, J. A. (1980).
                <em>A circumplex model of affect</em>.
                Journal of Personality and Social Psychology, 39(6), 1161-1178.
                <a href="http://pdodds.w3.uvm.edu/research/papers/others/1980/russell1980a.pdf" target="_blank">Link</a>
              </li>
              <li>
                Bradley, M. M., & Lang, P. J. (1994).
                <em>Measuring emotion: the Self-Assessment Manikin and the Semantic Differential</em>.
                Journal of Behavior Therapy and Experimental Psychiatry.
              </li>
              <li>
                Lang, P. J., Bradley, M. M., & Cuthbert, B. N. (1997).
                <em>International Affective Picture System (IAPS): Technical manual and affective ratings</em>.
                NIMH Center for the Study of Emotion and Attention, 1(39-58), 3.
              </li>
              <li>
                Warriner, A. B., Kuperman, V., & Brysbaert, M. (2013).
                <em>Norms of valence, arousal, and dominance for 13,915 English lemmas</em>.
                Behavior Research Methods, 45, 1191-1207.
                <a href="https://norare.clld.org/contributions/Warriner-2013-AffectiveRatings" target="_blank">Link
                  1</a>,
                <a href="https://github.com/JULIELab/XANEW/tree/master" target="_blank">Link 2</a>
              </li>
              <li>
                Marchewka, A., Żurawski, Ł., Jednoróg, K., & Grabowska, A. (2014).
                <em>The Nencki Affective Picture System (NAPS): Introduction to a novel, standardized, wide-range,
                  high-quality, realistic picture database</em>.
                Behavior Research Methods, 46, 596-610.
              </li>
              <li>
                Radford, A., Metz, L., & Chintala, S. (2015).
                <em>Unsupervised representation learning with deep convolutional generative adversarial
                  networks</em>.
                <a href="https://arxiv.org/abs/1511.06434" target="_blank">arXiv:1511.06434</a>.
              </li>

              <li>
                Kurdi, B., Lozano, S., & Banaji, M. R. (2017).
                <em>Introducing the Open Affective Standardized Image Set (OASIS)</em>.
                Behavior Research Methods, 49(2), 457-470.
                <a href="https://link.springer.com/article/10.3758/s13428-016-0715-3" target="_blank">Link</a>
              </li>
              <li>
                Shen, Y., Gu, J., Tang, X., & Zhou, B. (2020).
                <em>Interpreting the latent space of GANs for semantic face editing</em>.
                In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp.
                9243–9252).
              </li>
              <li>
                Härkönen, E., Hertzmann, A., Lehtinen, J., & Paris, S. (2020).
                <em>Ganspace: Discovering interpretable GAN controls</em>.
                In <em>Advances in Neural Information Processing Systems</em>, 33, 9841–9850.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>


</body>

</html>
